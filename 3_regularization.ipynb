{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed at run time \n",
    "    # with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 19.348890\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 11.2%\n",
      "Minibatch loss at step 500: 2.915113\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 74.4%\n",
      "Minibatch loss at step 1000: 1.610725\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 1500: 1.062804\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 2000: 0.944257\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 2500: 0.771967\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 79.6%\n",
      "Minibatch loss at step 3000: 0.762078\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 81.5%\n",
      "\n",
      "Test accuracy: 88.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('')\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best L2 regularization weight with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_weights = [1e-4, 1e-3, 1e-2, 1e-1]  # test different regularization values\n",
    "accuracy_values = []\n",
    "\n",
    "for regul_weight in regul_weights:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul_weight}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_values.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAFtCAYAAADBM4kgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpIQBjcREZGEMLiJiIgkhMFNREQkIf8P0y8/658KOdkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1324cd630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "x_ticks = np.array([str(i) for i in regul_weights])\n",
    "x_values = np.array(range(4))\n",
    "y_values = np.array(accuracy_values)\n",
    "\n",
    "sns.regplot(x_values, y_values, fit_reg=False)\n",
    "plt.xticks(x_values, x_ticks)\n",
    "plt.title('Performance with Different Regularization Weights')\n",
    "plt.xlabel('Regularization Weight')\n",
    "plt.ylabel('Test Accuracy')\n",
    "for i, label in enumerate(y_values):\n",
    "    plt.annotate(label, (x_values[i], y_values[i]))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best L2 regularization weight is 1e-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add L2 regularization to 2-layer neural network with regularization weight = 1e-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "h1_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed at run time with a \n",
    "    # training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, h1_nodes]))\n",
    "    biases = tf.Variable(tf.zeros([h1_nodes]))\n",
    "    h1_weights = tf.Variable(tf.truncated_normal([h1_nodes, num_labels]))\n",
    "    h1_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Add a hidden layer.\n",
    "    h1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(h1_train, h1_weights) + h1_biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "        beta_regul * (tf.nn.l2_loss(weights) + tf.nn.l2_loss(h1_weights))\n",
    "      \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    h1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(h1_valid, h1_weights) + h1_biases)\n",
    "    h1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(h1_test, h1_weights) + h1_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 667.204041\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 25.9%\n",
      "Minibatch loss at step 500: 196.421997\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 1000: 114.888908\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 1500: 68.742538\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2000: 41.441113\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 2500: 25.154388\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 3000: 15.421886\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.1%\n",
      "\n",
      "Test accuracy: 92.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('')\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get test accuracy above 90% which is a great improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an overfitting 2-layer neural network by using a small training set of size 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 622.373840\n",
      "Minibatch accuracy: 17.2%\n",
      "Validation accuracy: 23.4%\n",
      "Minibatch loss at step 500: 190.552353\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 1000: 115.562492\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.5%\n",
      "Minibatch loss at step 1500: 70.083298\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 2000: 42.502197\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 2500: 25.776297\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 3000: 15.633673\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.6%\n",
      "\n",
      "Test accuracy: 82.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "small_train_dataset = train_dataset[:500, :]\n",
    "small_train_labels = train_labels[:500]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (small_train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = small_train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = small_train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('')\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, training accuracy gets as high as 100% while validation and test accuracy are both lower than using larger training set in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "h1_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed at run time with a \n",
    "    # training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, h1_nodes]))\n",
    "    biases = tf.Variable(tf.zeros([h1_nodes]))\n",
    "    h1_weights = tf.Variable(tf.truncated_normal([h1_nodes, num_labels]))\n",
    "    h1_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Add a hidden layer.\n",
    "    h1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    \n",
    "    # Add dropout.\n",
    "    h1_drop = tf.nn.dropout(h1_train, keep_prob)\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(h1_drop, h1_weights) + h1_biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "        beta_regul * (tf.nn.l2_loss(weights) + tf.nn.l2_loss(h1_weights))\n",
    "      \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    h1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(h1_valid, h1_weights) + h1_biases)\n",
    "    \n",
    "    h1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(h1_test, h1_weights) + h1_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 854.651184\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 28.6%\n",
      "Minibatch loss at step 500: 191.848129\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 1000: 116.364662\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 1500: 70.592667\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 2000: 42.820351\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.1%\n",
      "Minibatch loss at step 2500: 25.971621\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 3000: 15.751930\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.7%\n",
      "\n",
      "Test accuracy: 84.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "small_train_dataset = train_dataset[:500, :]\n",
    "small_train_labels = train_labels[:500]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (small_train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = small_train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = small_train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3, keep_prob : 0.5}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('')\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 0.5 dropout, the overfitting case improves on getting higher validation and test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "h1_nodes = 1024\n",
    "h2_nodes = 300\n",
    "h3_nodes = 50\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed at run time with a \n",
    "    # training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, h1_nodes], \n",
    "                                              stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases = tf.Variable(tf.zeros([h1_nodes]))\n",
    "    h1_weights = tf.Variable(tf.truncated_normal([h1_nodes, h2_nodes], stddev=np.sqrt(2.0 / h1_nodes)))\n",
    "    h1_biases = tf.Variable(tf.zeros([h2_nodes]))\n",
    "    h2_weights = tf.Variable(tf.truncated_normal([h2_nodes, h3_nodes], stddev=np.sqrt(2.0 / h2_nodes)))\n",
    "    h2_biases = tf.Variable(tf.zeros([h3_nodes]))\n",
    "    h3_weights = tf.Variable(tf.truncated_normal([h3_nodes, num_labels], stddev=np.sqrt(2.0 / h3_nodes)))\n",
    "    h3_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    \n",
    "    # Add 3 hidden layers.\n",
    "    h1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    h2_train = tf.nn.relu(tf.matmul(h1_train, h1_weights) + h1_biases)\n",
    "    h3_train = tf.nn.relu(tf.matmul(h2_train, h2_weights) + h2_biases)\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(h3_train, h3_weights) + h3_biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * \\\n",
    "        (tf.nn.l2_loss(weights) + tf.nn.l2_loss(h1_weights) + tf.nn.l2_loss(h2_weights) + tf.nn.l2_loss(h3_weights))  \n",
    "      \n",
    "    # Optimizer. Add decaying learning rate.\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.5\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 9000, 0.8, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    h1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    h2_valid = tf.nn.relu(tf.matmul(h1_valid, h1_weights) + h1_biases)\n",
    "    h3_valid = tf.nn.relu(tf.matmul(h2_valid, h2_weights) + h2_biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(h3_valid, h3_weights) + h3_biases)\n",
    "    \n",
    "    h1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    h2_test = tf.nn.relu(tf.matmul(h1_test, h1_weights) + h1_biases)\n",
    "    h3_test = tf.nn.relu(tf.matmul(h2_test, h2_weights) + h2_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(h3_test, h3_weights) + h3_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.469083\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 29.4%\n",
      "Minibatch loss at step 500: 1.284745\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 1000: 0.929085\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 1500: 0.743571\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 2000: 0.669983\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 2500: 0.508353\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 3000: 0.593374\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 3500: 0.645039\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 4000: 0.465803\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 4500: 0.428971\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 5000: 0.451512\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 5500: 0.349372\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 6000: 0.599559\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 6500: 0.529129\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 7000: 0.515168\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 7500: 0.432416\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 8000: 0.442485\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 8500: 0.474586\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 9000: 0.565976\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 88.3%\n",
      "\n",
      "Test accuracy: 94.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001  # when decreasing learning rate, number of steps should be increased\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('')\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 3 hidden layers and decaying learning rate, we can get test accuracy to around 94%. Try to play with the hyperparameters to get the best result."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
