{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed at run time \n",
    "    # with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 22.845243\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 12.0%\n",
      "Minibatch loss at step 500: 3.018713\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 74.6%\n",
      "Minibatch loss at step 1000: 1.783425\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 1500: 1.150003\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 2000: 0.947801\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 80.0%\n",
      "Minibatch loss at step 2500: 0.802964\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 79.5%\n",
      "Minibatch loss at step 3000: 0.763191\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.1%\n",
      "\n",
      "Test accuracy: 88.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('')\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best L2 regularization weight with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_weights = [1e-4, 1e-3, 1e-2, 1e-1]  # test different regularization values\n",
    "accuracy_values = []\n",
    "\n",
    "for regul_weight in regul_weights:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul_weight}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracy_values.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAFtCAYAAADBM4kgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wfLD+mDtT99VQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x132633d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "x_ticks = np.array([str(i) for i in regul_weights])\n",
    "x_values = np.array(range(4))\n",
    "y_values = np.array(accuracy_values)\n",
    "\n",
    "sns.regplot(x_values, y_values, fit_reg=False)\n",
    "plt.xticks(x_values, x_ticks)\n",
    "plt.title('Performance with Different Regularization Weights')\n",
    "plt.xlabel('Regularization Weight')\n",
    "plt.ylabel('Test Accuracy')\n",
    "for i, label in enumerate(y_values):\n",
    "    plt.annotate(label, (x_values[i], y_values[i]))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best L2 regularization weight is 1e-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add L2 regularization to 2-layer neural network with regularization weight = 1e-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "h1_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed at run time with a \n",
    "    # training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, h1_nodes]))\n",
    "    biases = tf.Variable(tf.zeros([h1_nodes]))\n",
    "    h1_weights = tf.Variable(tf.truncated_normal([h1_nodes, num_labels]))\n",
    "    h1_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Add a hidden layer.\n",
    "    h1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(h1_train, h1_weights) + h1_biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "        beta_regul * (tf.nn.l2_loss(weights) + tf.nn.l2_loss(h1_weights))\n",
    "      \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    h1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(h1_valid, h1_weights) + h1_biases)\n",
    "    h1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(h1_test, h1_weights) + h1_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 668.061157\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 29.4%\n",
      "Minibatch loss at step 500: 193.292816\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 1000: 114.451912\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 1500: 68.655083\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 2000: 41.405796\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 2500: 25.154655\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 3000: 15.401735\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.4%\n",
      "\n",
      "Test accuracy: 92.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('')\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get test accuracy above 90% which is a great improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an overfitting 2-layer neural network by using a small training set of size 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 606.428101\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 30.8%\n",
      "Minibatch loss at step 500: 190.202942\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 1000: 115.349838\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 1500: 69.954491\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 2000: 42.423779\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.0%\n",
      "Minibatch loss at step 2500: 25.728626\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 72.9%\n",
      "Minibatch loss at step 3000: 15.604310\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 73.3%\n",
      "\n",
      "Test accuracy: 82.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "small_train_dataset = train_dataset[:500, :]\n",
    "small_train_labels = train_labels[:500]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (small_train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = small_train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = small_train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('')\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, training accuracy gets as high as 100% while validation and test accuracy are both lower than using larger training set in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "h1_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed at run time with a \n",
    "    # training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, h1_nodes]))\n",
    "    biases = tf.Variable(tf.zeros([h1_nodes]))\n",
    "    h1_weights = tf.Variable(tf.truncated_normal([h1_nodes, num_labels]))\n",
    "    h1_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Add a hidden layer.\n",
    "    h1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    \n",
    "    # Add dropout.\n",
    "    h1_drop = tf.nn.dropout(h1_train, keep_prob)\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(h1_drop, h1_weights) + h1_biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + \\\n",
    "        beta_regul * (tf.nn.l2_loss(weights) + tf.nn.l2_loss(h1_weights))\n",
    "      \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    h1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(h1_valid, h1_weights) + h1_biases)\n",
    "    \n",
    "    h1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(h1_test, h1_weights) + h1_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 903.354248\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 28.5%\n",
      "Minibatch loss at step 500: 191.025894\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 1000: 115.935913\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.2%\n",
      "Minibatch loss at step 1500: 70.329597\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 2000: 42.665131\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 2500: 25.876102\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 3000: 15.693745\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 75.8%\n",
      "\n",
      "Test accuracy: 83.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "small_train_dataset = train_dataset[:500, :]\n",
    "small_train_labels = train_labels[:500]\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (small_train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = small_train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = small_train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3, keep_prob : 0.5}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('')\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 0.5 dropout, the overfitting case improves on getting higher validation and test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "h1_nodes = 1024\n",
    "h2_nodes = 300\n",
    "h3_nodes = 50\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed at run time with a \n",
    "    # training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, h1_nodes], \n",
    "                                              stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases = tf.Variable(tf.zeros([h1_nodes]))\n",
    "    h1_weights = tf.Variable(tf.truncated_normal([h1_nodes, h2_nodes], stddev=np.sqrt(2.0 / h1_nodes)))\n",
    "    h1_biases = tf.Variable(tf.zeros([h2_nodes]))\n",
    "    h2_weights = tf.Variable(tf.truncated_normal([h2_nodes, h3_nodes], stddev=np.sqrt(2.0 / h2_nodes)))\n",
    "    h2_biases = tf.Variable(tf.zeros([h3_nodes]))\n",
    "    h3_weights = tf.Variable(tf.truncated_normal([h3_nodes, num_labels], stddev=np.sqrt(2.0 / h3_nodes)))\n",
    "    h3_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Add 3 hidden layers with dropout.\n",
    "    h1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights) + biases)\n",
    "    h2_train = tf.nn.relu(tf.matmul(h1_train, h1_weights) + h1_biases)\n",
    "    h3_train = tf.nn.relu(tf.matmul(h2_train, h2_weights) + h2_biases)\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(h3_train, h3_weights) + h3_biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_regul * \\\n",
    "        (tf.nn.l2_loss(weights) + tf.nn.l2_loss(h1_weights) + tf.nn.l2_loss(h2_weights) + tf.nn.l2_loss(h3_weights))  \n",
    "      \n",
    "    # Optimizer. Add decaying learning rate.\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    starter_learning_rate = 0.5\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 1000, 0.8, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    h1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    h2_valid = tf.nn.relu(tf.matmul(h1_valid, h1_weights) + h1_biases)\n",
    "    h3_valid = tf.nn.relu(tf.matmul(h2_valid, h2_weights) + h2_biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(h3_valid, h3_weights) + h3_biases)\n",
    "    \n",
    "    h1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "    h2_test = tf.nn.relu(tf.matmul(h1_test, h1_weights) + h1_biases)\n",
    "    h3_test = tf.nn.relu(tf.matmul(h2_test, h2_weights) + h2_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(h3_test, h3_weights) + h3_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.451322\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 39.0%\n",
      "Minibatch loss at step 500: 1.306375\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 1000: 0.977216\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1500: 0.752406\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 2000: 0.694403\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2500: 0.522324\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 3000: 0.563780\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 3500: 0.655389\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 4000: 0.468421\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 4500: 0.393021\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 5000: 0.418853\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 5500: 0.316852\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 6000: 0.485946\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 6500: 0.418398\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 7000: 0.479347\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 7500: 0.370312\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 8000: 0.339682\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 8500: 0.390620\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 9000: 0.399453\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.5%\n",
      "\n",
      "Test accuracy: 95.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001  # when decreasing learning rate, number of steps should be increased\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('')\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 3 hidden layers and decaying learning rate, we can get test accuracy to around 95.6%."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
